\chapter{Аналитический раздел}

\section{Постановка задачи}

В соответствии с техническим заданием на курсовую работу необходимо разработать загружаемый модуль, предоставляющий пользователю информацию о работе сетевой подсистемы Linux. Для решения поставленной задачи необходимо:
\begin{itemize}[label=---]
	\item произвести анализ функций и структур, используемых для обработки сетевых кадров;
	\item разработать загружаемый модуль ядра, предоставляющий информацию о работе сетевой подсистемы;
	\item реализовать модуль ядра;
	\item протестировать работу реализованного загружаемого модуля.
\end{itemize}

\section{Взаимодействие сетевой карты и сетевой подсистемы}

Почти все устройства взаимодействуют с ядром одним из двух способов: опрос и прерывания. Также на практике может применяться комбинация этих методов~\cite{net}.

При \textbf{опросе} ядро постоянно проверяет, есть ли у устройства какие--то данные для передачи. Оно может делать это, например, постоянно считывая регистр памяти на устройстве или переодически, по истечению таймера, проводить проверку. Такой подход требует большого количества системных ресурсов и редко применяется на практике.

При использовании \textbf{прерываний} устройство генерирует аппаратный сигнал при возникновении определённых событий. Каждое прерывание запускает функцию, называемую обработчиком прерываний, которая должна быть совместима с
устройством, следовательно, она регистрируется драйвером устройства при его загрузке. Для идентификации обработчика ядру нужны как номер IRQ, так и идентификатор устройства. Это нужно, так как IRQ может совместно использоваться несколькими устройствами при определённых условиях.

При прерывании сетевая карта может сообщить своему драйверу несколько разных вещей. Среди них:
\begin{itemize}[label=---]
	\item получение кадра;
	\item сбой передачи;
	\item передача DMA успешно завершена --- получив кадр для отправки, буфер, в котором он хранится, освобождается драйвером, как только кадр загружается в память сетевой карты для передачи. При синхронных передачах (без DMA) драйвер сразу узнает, когда кадр был загружен на сетевую карту. Но при использовании DMA, который использует асинхронные передачи, драйверу устройства необходимо дождаться явного прерывания от сетевой карты;
	\item устройство имеет достаточно памяти для обработки новой передачи --- драйвер сетевого устройства обычно отключает передачу, останавливая очередь на выход, когда в этой очереди недостаточно свободного места.
\end{itemize}

Этот метод представляет собой наилучший вариант при низких нагрузках на трафик. Но он плохо работает при высокой нагрузке, потому что обработка прерываний для обслуживания каждого кадра по отдельности может занять большую часть времени.

Большое количество драйверов обрабатывают сразу несколько кадров при прерывании. Обработчик, зарегистрированный драйвером, загружает кадры и помещает их в очередь ввода ядра. Подобным образом функционирует NAPI.

Прерывания, управляемые таймером это метод, который является усовершенствованием предыдущих. Вместо того, чтобы устройство асинхронно уведомляло драйвер о приёме кадра, прерывания генерируются с определённым интервалом. Затем обработчик проверит, поступили ли какие-либо кадры после предыдущего прерывания, и обработает их все за один раз.

\section{Обработка прерываний}

Всякий раз, когда процессор получает прерывании, он вызывает обработчик, связанный с этим прерыванием. Во время выполнения обработчика, которой выполняется в контексте прерывания, другие прерывания отключаются для этого процессора. Это означает, что если процессор занят обслуживанием одного прерывания, он не может обслуживать другие. Он также не может выполнять какой--либо другой процесс. Такой выбор дизайна помогает снизить вероятность возникновения условий гонки. Однако такие жёсткие ограничения на работу процессора серьёзно влияют на производительность системы. Следовательно, работа, выполняемая обработчиками прерываний, должна быть как можно более быстрой. Объем работы обработчика  зависит от типа события, иногда нужно просто сохранить код нажатой клавиши, а в другом случае действия не являются тривиальными, и их выполнение может потребовать много процессорного времени. У драйверов сетевых устройств относительно сложная работа: им нужно выделить буфер (sk\_buff), скопировать в него полученные данные, инициализировать несколько параметров в структуре буфера для обработчиков протоколов более высокого уровня и передать дальше по цепочке обслуживания~\cite{net}. 

По этой причине современные обработчики прерываний делятся на верхнюю и нижнюю половины. Верхняя половина состоит из всего, что должно быть выполнено перед освобождением процессора, как правило это загрузки данных, необходимых для дальнейшей обработки. Нижняя половина содержит всё остальное, то есть выполняет основную часть работы по обработке прерывания. Нижнюю половину можно определить как асинхронный запрос на выполнение определённой
функции. Следующая модель обработки прерываний позволяет ядру отключать прерывания на гораздо меньшее время:
\begin{itemize}[label=---]
	\item устройство генерирует сигнал прерывания;
	\item процессор выполняет верхнюю половину, блокируя прерывания, как правило она делает следующее: сохраняет в оперативной памяти всю информацию, которая позже понадобится нижней половине, планирует на выполнение нижнюю половину и разрешает прерывания;
	\item позднее выполняется нижняя половина прерывания, содержащая основной объём работы, но уже не в контексте прерывания.
\end{itemize}

Самым большим улучшением между ядрами 2.2 и 2.4, стало внедрение программных прерываний (softirqs), которые можно рассматривать как многопоточную версию обработчиков нижней половины. Многие softirq могут выполняться конкурентно, но также один и тот же softirq может выполняться конкурентно. Единственное ограничение на параллелизм заключается в том, что только один экземпляр каждого softirq может выполняться одновременно. Есть всего 6 типов softirq:
\begin{itemize}[label=---]
	\item HI\_SOFTIRQ;
	\item TIMER\_SOFTIRQ;
	\item NET\_TX\_SOFTIRQ;
	\item NET\_RX\_SOFTIRQ;
	\item SCSI\_SOFTIRQ;
	\item TASKLET\_SOFTIRQ.
\end{itemize}

Для их выполнения в системе запускаются потоки ksoftirqd, по одному на CPU, которые крутятся в цикле в ожидание поступления работы. При наличии запланированных на выполнение нижних половин прерываний вызывается функция do\_softirq, которая и выполняет зарегистрированный обработчик. Сама функция do\_softirq, проверив, что сейчас не обрабатываются прерывания, проверяет наличие softirq на выполнение и переходит выполнению обработчика (функция \_\_do\_softirq). Проходятся по битовой маске в цикле, определяются softirq требующие выполнения и запускаются зарегистрированные обработчики хранящиеся в массиве softirq\_vec~\cite{guide}. 

В сетевой подсистеме NET\_RX\_SOFTIRQ используется для обработки входящего трафика (net\_rx\_action), а NET\_TX\_SOFTIRQ исходящего (net\_tx\_action).

Каждый CPU имеет свою собственную структуру данных для управления входящим и исходящим трафиком. Это структура  softnet\_data, которая представлена в листинге \ref{lst:softnet_data}~\cite{softnet_data}.

\begin{center}
	\captionsetup{justification=raggedright,singlelinecheck=off}
	\begin{lstlisting}[label=lst:softnet_data,caption=Структуры softnet\_data]
struct softnet_data {
	struct list_head	poll_list;
	struct sk_buff_head	process_queue;
	unsigned int		processed;
	unsigned int		time_squeeze;
	#ifdef CONFIG_RPS
	struct softnet_data	*rps_ipi_list;
	#endif
	bool			in_net_rx_action;
	bool			in_napi_threaded_poll;
	#ifdef CONFIG_NET_FLOW_LIMIT
	struct sd_flow_limit __rcu *flow_limit;
	#endif
	struct Qdisc		*output_queue;
	struct Qdisc		**output_queue_tailp;
	struct sk_buff		*completion_queue;
	#ifdef CONFIG_XFRM_OFFLOAD
	struct sk_buff_head	xfrm_backlog;
	#endif
	struct {
		u16 recursion;
		u8  more;
		#ifdef CONFIG_NET_EGRESS
		u8  skip_txqueue;
		#endif
	} xmit;
	#ifdef CONFIG_RPS
	unsigned int		input_queue_head ____cacheline_aligned_in_smp;
	call_single_data_t	csd ____cacheline_aligned_in_smp;
	struct softnet_data	*rps_ipi_next;
	unsigned int		cpu;
	unsigned int		input_queue_tail;
	#endif
	unsigned int		received_rps;
	unsigned int		dropped;
	struct sk_buff_head	input_pkt_queue;
	struct napi_struct	backlog;
	spinlock_t		defer_lock ____cacheline_aligned_in_smp;
	int			defer_count;
	int			defer_ipi_scheduled;
	struct sk_buff		*defer_list;
	call_single_data_t	defer_csd;
};		
	\end{lstlisting}
\end{center}
\FloatBarrier

Структура включает в себя как поля, используемые для приёма, так и поля, используемые для передачи. Не все драйвера используют NAPI, но всем они используют эту структуру. Рассмотрим некоторые поля подробнее:
\begin{itemize}[label=---]
	\item poll\_list --- двунаправленный список NAPI--структур;
	\item process\_queue --- очередь кадров обрабатываемая в process\_backlog;
	\item processed --- количество обработанных кадров;
	\item time\_squeeze --- оличество раз, когда у net\_rx\_action была работа, но бюджета не хватало либо было достигнуто ограничение по времени, прежде чем работа была завершена;
	\item in\_net\_rx\_action --- флаг о том, что данный экземпляр структуры в текущей момент обрабатывается функцией  net\_rx\_action;
	\item flow\_limit --- поле, хранящее данные о ограничении потоков RPS;
	\item output\_queue --- очередь, хранящая кадры для отправки;
	\item completion\_queue --- список буферов данных, которые были успешно переданы и, следовательно, могут быть освобождены;
	\item received\_rps --- количество раз, когда посредством межпроцессорного прерывания будили CPU для обработки пакетов;
	\item dropped --- количество отброшенных кадров;
	\item input\_pkt\_queue --- очередь, где сохраняются входящие кадры перед обработкой драйвером. Она используется драйверами, не использующими NAPI, или как backlog--очередь. Драйвера с NAPI используют свои собственные частные очереди;
	\item backlog --- NAPI--структура для обработки backlog--очереди.
\end{itemize}

\section{Механизм NAPI}

New Api (NAPI) был создан в качестве механизма снижения количества прерываний, генерируемых сетевыми устройствами по мере прибытия пакетов. Он позволяет драйверу устройства регистрировать функцию poll, вызываемую подсистемой NAPI для сбора данных~\cite{rec}.

Основная идея реализованная в NAPI заключается в комбинации методов прерывания и опроса. Если новые кадры получены, когда ядро ещё не завершило обработку предыдущих, нет необходимости в генерации новых прерывание, можно просто продолжать обрабатывать все, что находится в очереди ввода устройства с отключёнными прерываниями для устройства, и повторно включать прерывания, как только очередь опустеет. Таким образом,  используются преимущества как прерываний, так и опроса~\cite{net}:
\begin{itemize}[label=---]
	\item асинхронные события, такие как приём одного или нескольких кадров, обозначаются прерываниями, так что ядру не нужно постоянно проверять, пуста ли очередь входа устройства;
	\item если в очереди входа устройства что--то осталось, не нужно заново генерировать прерывания и тратить время на их обработку.
\end{itemize}

Алгоритм использования NAPI драйверами сетевых устройств выглядит так:
\begin{itemize}[label=---]
	\item драйвер включает NAPI, изначально тот находится в неактивном состоянии;
	\item прибывает кадр и сетевая карта напрямую отправляет его в память;
	\item сетевая карта генерирует IRQ, запуская обработчика прерываний;
	\item обработчик будит подсистему NAPI с помощью SoftIRQ, которая начинает собирать пакеты вызывая зарегистрированную драйвером функцию poll;
	\item драйвер отключает последующие генерирования прерываний сетевой картой, чтобы позволить подсистеме NAPI обрабатывать пакеты без помех со стороны устройства;
	\item когда вся работа выполнена, подсистема NAPI отключается, а генерирование прерываний устройством включается снова.
\end{itemize}

Этот метод сбора данных позволил уменьшить нагрузку на процессор по сравнению со старым методом, поскольку несколько кадров могут одновременно приниматься без необходимости генерирования IRQ для каждого из них. Драйвер устройства реализует функцию poll и регистрирует её с помощью NAPI.

\section{Получение данных}

Высокоуровневый путь, по которому проходит кадр от прибытия до приёмного буфера сокета выглядит так:
\begin{itemize}[label=---]
	\item драйвер загружается и инициализируется;
	\item кадр прибывает из сети в сетевую карту;
	\item кадр копируется посредством DMA в кольцевой буфер памяти ядра;
	\item генерируется аппаратное прерывание, чтобы система узнала о появлении пакета в памяти;
	\item обработчик вызывает NAPI, чтобы начать цикл опроса (poll loop), если он ещё не начат;
	\item очищаются те области памяти в кольцевом буфере, в которые были записаны сетевые данные;
	\item данные передаются для дальнейшей обработки на сетевой уровень в виде sk\_buff;
	\item если включено управление пакетами, или если в сетевой карте есть несколько очередей приёма, то фреймы входящих сетевых данных распределяются по нескольким CPU системы.
\end{itemize}

При получении кадра на сетевой карте генерируется прерывание. В самом обработчике вызывается функция napi\_schedule, в которую как параметр предаётся napi\_struct драйвера. Её код представлен в листинге~\ref{lst:napi_schedule}~\cite{napi_schedule}.

\begin{center}
	\captionsetup{justification=raggedright,singlelinecheck=off}
	\begin{lstlisting}[label=lst:napi_schedule,caption=Функция \_\_\_\_napi\_schedule,showstringspaces=false]
static inline void ____napi_schedule(struct softnet_data *sd,
struct napi_struct *napi)
{
	struct task_struct *thread;
	
	lockdep_assert_irqs_disabled();
	
	if (test_bit(NAPI_STATE_THREADED, &napi->state)) {
		thread = READ_ONCE(napi->thread);
		if (thread) {
			if (READ_ONCE(thread->__state) != TASK_INTERRUPTIBLE)
			set_bit(NAPI_STATE_SCHED_THREADED, &napi->state);
			wake_up_process(thread);
			return;
		}
	}
	
	list_add_tail(&napi->poll_list, &sd->poll_list);
	WRITE_ONCE(napi->list_owner, smp_processor_id());

	if (!sd->in_net_rx_action)
	__raise_softirq_irqoff(NET_RX_SOFTIRQ);
}
	\end{lstlisting}
\end{center}
\FloatBarrier

Помимо пробуждения треда обработки NAPI в этой функции в конец очереди poll\_list структуры  softnet\_data добавляется структура  napi\_struct, код которой представлен в листинге~\ref{lst:napi_struct}~\cite{napi_struct}, драйвера, содержащая данные, необходимую для обработки пришедших на устройство кадров. Также планируется на выполнение нижняя часть прерывания NET\_RX\_SOFTIRQ, обработчиком  которой является функция net\_rx\_action. Её код представлен в листинге~\ref{lst:net_rx_action}~\cite{net_rx_action}.

\begin{center}
	\captionsetup{justification=raggedright,singlelinecheck=off}
	\begin{lstlisting}[label=lst:napi_struct,caption=Структура napi\_struct,showstringspaces=false]
struct napi_struct {
	struct list_head	poll_list;
	
	unsigned long		state;
	int			weight;
	int			defer_hard_irqs_count;
	unsigned long		gro_bitmask;
	int			(*poll)(struct napi_struct *, int);
	#ifdef CONFIG_NETPOLL
	int			poll_owner;
	#endif
	int			list_owner;
	struct net_device	*dev;
	struct gro_list		gro_hash[GRO_HASH_BUCKETS];
	struct sk_buff		*skb;
	struct list_head	rx_list; /* Pending GRO_NORMAL skbs */
	int			rx_count; /* length of rx_list */
	unsigned int		napi_id;
	struct hrtimer		timer;
	struct task_struct	*thread;
	struct list_head	dev_list;
	struct hlist_node	napi_hash_node;
};
	\end{lstlisting}
\end{center}
\FloatBarrier

Рассмотрим некоторые поля подробнее:
\begin{itemize}[label=---]
	\item poll\_list --- поддерживает двунаправленный список NAPI--структур;
	\item poll --- функция опроса, зарегистрированная драйвером; 
	\item weight --- максимальное количество кадров, которое может быть обработано за один раз;
	\item dev --- дескриптор сетевого устройства.
\end{itemize}

\begin{center}
	\captionsetup{justification=raggedright,singlelinecheck=off}
	\begin{lstlisting}[label=lst:net_rx_action,caption=Функция net\_rx\_action,showstringspaces=false]
static __latent_entropy void net_rx_action(struct softirq_action *h)
{
	struct softnet_data *sd = this_cpu_ptr(&softnet_data);
	unsigned long time_limit = jiffies +
	usecs_to_jiffies(READ_ONCE(netdev_budget_usecs));
	int budget = READ_ONCE(netdev_budget);
	LIST_HEAD(list);
	LIST_HEAD(repoll);
	
	start:
	sd->in_net_rx_action = true;
	local_irq_disable();
	list_splice_init(&sd->poll_list, &list);
	local_irq_enable();
	
	for (;;) {
		struct napi_struct *n;
		
		skb_defer_free_flush(sd);
		
		if (list_empty(&list)) {
			if (list_empty(&repoll)) {
				sd->in_net_rx_action = false;
				barrier();
				if (!list_empty(&sd->poll_list))
				goto start;
				if (!sd_has_rps_ipi_waiting(sd))
				goto end;
			}
			break;
		}
		
		n = list_first_entry(&list, struct napi_struct, poll_list);
		budget -= napi_poll(n, &repoll);
		
		if (unlikely(budget <= 0 ||
		time_after_eq(jiffies, time_limit))) {
			sd->time_squeeze++;
			break;
		}
	}
	
	local_irq_disable();
	
	list_splice_tail_init(&sd->poll_list, &list);
	list_splice_tail(&repoll, &list);
	list_splice(&list, &sd->poll_list);
	if (!list_empty(&sd->poll_list))
	__raise_softirq_irqoff(NET_RX_SOFTIRQ);
	else
	sd->in_net_rx_action = false;
	
	net_rps_action_and_irq_enable(sd);
	end:;
}
	\end{lstlisting}
\end{center}
\FloatBarrier

Функция итерируется по списку структур NAPI, стоящих в очереди текущего CPU. Цикл обработки ограничен объём и временем работы. Таким образом ядро не позволяет обработке пакетов занять все ресурсы процессора. budget — это весь доступный бюджет, который будет разделён на все доступные NAPI--структуры, зарегистрированные на этот CPU. Бюджет является настраиваемой величиной.

Выбрав NAPI--структуру (napi\_struct)  вызывается функция poll, которая возвращает количество обработанных кадров. Сама функция функция собирает сетевые данные и отправляет их для дальнейшей обработки. Затем это количество вычитается из общего бюджета. Если драйверная функция poll расходует весь свой вес (64), она не должна изменять состояние NAPI и эта структура будет добавлена в конец poll\_list.

Выход из цикла net\_rx\_action будет совершён, если: список poll, зарегистрированный для данного CPU, больше не содержит NAPI--структур, остаток бюджета <= 0 или был достигнут временной предел в два jiffies. Если были обработаны не все NAPI--структуры, то тогда заново планируется на выполнение NET\_RX\_SOFTIRQ. Прежде чем выполнить возврат из net\_rx\_action вызывается net\_rps\_action\_and\_irq\_enable. Если включено управление принимаемыми пакетами (RPS) то эта функция пробуждает удалённые CPU, чтобы они начали обрабатывать сетевые данные.

Generic Receive Offloading (GRO) — это программная реализация аппаратной оптимизации, известной как Large Receive Offloading (LRO). Суть обоих механизмов в том, чтобы уменьшить количество пакетов, передаваемых по сетевому стеку, за счёт комбинирования <<достаточно похожих>> пакетов. Это позволяет снизить нагрузку на CPU. Пусть передаётся большой файл, и большинство пакетов содержат чанки данных из этого файла. Вместо отправки по стеку маленьких кадров по одному, входящие кадры можно комбинировать в один большой. А затем уже передавать его по стеку. Таким образом обрабатывается заголовок одного пакета, при этом передавая данные нескольких маленьких. Но этой оптимизации присуща проблема потери информации. Если какой--то кадр имеет опцию или флаг, то они могут быть потеряны при объединении с другими пакетами.

Функция napi\_gro\_receive, вызываемая в poll функции драйвера, занимается обработкой сетевых данных для GRO, если включен, и отправкой их дальше по стеку. Большая часть логики находится в функции dev\_gro\_receive. В самой функции происходи проверка, можно ли объединить пакет с имеющимся потоком. Если пришло время сбросить GRO--пакет, то он передаётся далее по стеку посредством вызова netif\_receive\_skb. Если пакет не был объединён и в системе меньше MAX\_GRO\_SKBS (8) GRO-потоков, то в список gro\_list NAPI-структуры данного CPU добавляется новая запись. По завершении dev\_gro\_receive вызывается napi\_skb\_finish, которая освобождает экземпляры структур, невостребованные по причине слияния пакета, либо для передачи данных по сетевому стеку вызывается netif\_receive\_skb.

Некоторые сетевые карты на аппаратном уровне поддерживают несколько очередей. Это означает, что входящие пакеты могут напрямую отправляться в разные области памяти, выделенные для каждого очереди. При этом опрос каждой области выполняется с помощью отдельных NAPI-структур. Так что прерывания и пакеты будут обрабатываться несколькими CPU. Этот механизм называется Receive Side Scaling (RSS). Receive Packet Steering (RPS) --- это программная реализация RSS. А раз реализовано в коде, то может быть применено для любой сетевой карты, даже если она имеет лишь одну очередь приёма. RPS генерирует для входящих данных хэш, чтобы определить, какой CPU должен их обработать. Затем данные помещаются во входящую очередь (backlog) этого процессора в ожидании последующей обработки. В CPU передаётся межпроцессорное прерывание (IPI), инициирующее обработку очереди.

netif\_receive\_skb действует по разному, в зависимости от того, включён ли RPS. Если RPS выключен, то данные просто передаются дальше по сетевому стеку. Иначе выполняет ряд вычислений чтобы определить, backlog--очередь какого CPU нужно использовать. Для добавления в очередь используется функция enqueue\_to\_backlog.

Эта функция сначала получает указатель на структуру softnet\_data удалённого CPU, содержащую указатель на input\_pkt\_queue. Если привешен максимальный поток или длинна очереди, то данные отбрасываются. Пусть все проверки пройдены, тогда если очередь пустая: проверяется, запущен ли NAPI на удалённом CPU. Если нет, проверяется, находится ли в очереди на отправку IPI. Если нет, то IPI помещается в очередь, а посредством вызова \_\_\_\_napi\_schedule запускается цикл обработки NAPI. Если очередь не пуста, то данные сразу передаются в очередь.

Backlog--очереди каждого CPU используют NAPI так же, как и драйвер устройства. Предоставляется функция poll, используемая для обработки пакетов из контекста SoftIRQ. Как и в случае с драйвером, здесь тоже применяется weight. Эти очереди обслуживаются функцией process\_backlog, которая содержит цикл выполняемый до тех пор, пока его вес не будет израсходован или пока не останется больше данных. Данные вынимаются по частям из backlog--очереди и передаются в \_\_netif\_receive\_skb. Ветвь кода будет такой же, как и в случае с отключённым RPS. Поллер перезапускается посредством вызова \_\_\_\_napi\_schedule из enqueue\_to\_backlog для обработки backlog--очереди.

\section{Отправка данных}

Высокоуровневый путь, по которому проходит пакет при отправке выглядит так:
\begin{itemize}[label=---]
	\item данные записываются с помощью системного вызова;
	\item данные передаются вниз по сетевому стеку, заполняются поля sk\_buff;
	\item выбирается очередь вывода с помощью XPS или хэш--функции;
	\item данные попадают в соответствующую очередь (qdisc) устройства;
	\item qdisc либо передаст данные напрямую, если сможет, либо поставит их в очередь для отправки;
	\item драйвер создаёт необходимые отображения DMA, чтобы устройство могло считывать данные из оперативной памяти;
	\item драйвер сигнализирует устройству, что данные готовы к передаче;
	\item устройство извлекает данные из оперативной памяти и передаёт их;
	\item как только передача завершена, устройство инициирует прерывание, чтобы сигнализировать о завершении передачи.
\end{itemize}

Linux поддерживает механизм, называемый управлением трафиком. Эта функция позволяет пользователям контролировать передачу пакетов с компьютера. Система управления трафиком содержит несколько различных наборов дисциплин обслуживания (qdisc), которые предоставляют различные функции для управления транспортным потоком~\cite{send}. 

В Linux с каждым интерфейсом связан qdisc по умолчанию. Для сетевого оборудования, поддерживающего только одну очередь передачи, используется pfifo\_fast по умолчанию. Сетевое оборудование, поддерживающее несколько очередей передачи, использует mq по умолчанию.

Обработчики протоколов канального уровня, для отправки кадра вызывают функцию dev\_queue\_xmit. В которой дополнительно обрабатывается sk\_buff, чтобы можно было получить доступ к заголовку ethernet и устанавливается приоритет кадра. Далее определяется какую именно очередь передачи использовать, вызовом netdev\_pick\_tx.

Если имеется более одной очереди, то для определения её номера вызывается ndo\_select\_queue, реализуема драйвером для более оптимального выбора очереди, или \_\_netdev\_pick\_tx. В этой функции проверяется была ли очередь закеширована на сокете или нет. Если была, то возвращаем номер этой очереди. Иначе получаем через настройки XPS номер очереди, вызвав get\_xps\_queue. Если возвращает  -1, потому что это ядро не поддерживает XPS, или он не был настроен, или настроенное сопоставление ссылается на недопустимую очередь, вызовется skb\_tx\_hash, которая вычисляет хеш буфера, который и является номером очереди.

Управление передачей пакетов (XPS) --- это функция, которая позволяет пользователю определять, какие процессоры могут обрабатывать операции передачи для каждой очереди, поддерживаемой устройством. Цель этой функции в основном состоит в том, чтобы избежать блокировки соединений при обработке запросов на передачу.

Получив номер очереди, получаем на неё ссылку и добавляем буфер в очередь вызовом \_\_dev\_xmit\_skb, если такая операция определена, и переходим к концу функции. Единственными устройствами, которые могут иметь qdisc без очередей, являются устройства обратной связи и туннельные устройства.

В функции \_\_dev\_xmit\_skb проверятся отключена ли Qdisc, если отключена, то освобождаются данные и возвращается код ошибки. Иначе вызывается qdisc\_run для запуска обработки очереди.

В функция sch\_direct\_xmit, если очередь передачи не остановлена, то вызывается dev\_hard\_start\_xmit, которая отвечает за передачу данных с сетевого устройства. Код возврата из этой функции сохраняется и будет проверен далее функцией dev\_xmit\_complete, чтобы определить, была ли передача успешной. Если был возвращён код NETDEV\_TX\_BUSY, драйвер сейчас <<занят>> и не может отправить данные, то вызывается dev\_requeue\_skb, в которой данные встают в очередь для повторной отправки и планируется сама отправка (\_\_netif\_schedule). Иначе дополнительно выводится предупреждение.

Код функции \_\_qdisc\_run представлен в листинге~\ref{lst:__qdisc_run}~\cite{qdisc_run}.
\begin{center}
	\captionsetup{justification=raggedright,singlelinecheck=off}
	\begin{lstlisting}[label=lst:__qdisc_run,caption=Функция \_\_qdisc\_run,showstringspaces=false]
void __qdisc_run(struct Qdisc *q)
{
	int quota = READ_ONCE(dev_tx_weight);
	int packets;
	
	while (qdisc_restart(q, &packets)) {
		quota -= packets;
		if (quota <= 0) {
			if (q->flags & TCQ_F_NOLOCK)
			set_bit(__QDISC_STATE_MISSED, &q->state);
			else
			__netif_schedule(q);
			
			break;
		}
	}
}
	\end{lstlisting}
\end{center}
\FloatBarrier

Функция qdisc\_restart вызывает dequeue\_skb, чтобы получить следующий пакет для передачи. Если очередь пуста, qdisc\_restart вернёт значение false, что остановит цикл. Пусть есть данные для передачи, получаются ссылки на блокировку очереди qdisc, связанное с qdisc устройство и очередь передачи и передаются в sch\_direct\_xmit, чей код возврата и возвращается функцией. То есть в цикле постоянно пытаются передать данные в рамках квоты. Всё что не удалось отправить, планируется для отправки через \_\_netif\_schedule. Также dequeue\_skb в первую очередь возвращает кадры, которые когда--то пытались отправить, но не получилось, и они вернулись. Например, как при возврате кода NETDEV\_TX\_BUSY. Всё это ещё выполняется в рамках системного вызова для отправки данных, например, sendmsg.

Код функции \_\_netif\_reschedule представлен в листинге~\ref{lst:__netif_reschedule}~\cite{netif_reschedule}.
\begin{center}
	\captionsetup{justification=raggedright,singlelinecheck=off}
	\begin{lstlisting}[label=lst:__netif_reschedule,caption=Функция \_\_netif\_reschedule,showstringspaces=false]
static void __netif_reschedule(struct Qdisc *q)
{
	struct softnet_data *sd;
	unsigned long flags;
	
	local_irq_save(flags);
	sd = this_cpu_ptr(&softnet_data);
	q->next_sched = NULL;
	*sd->output_queue_tailp = q;
	sd->output_queue_tailp = &q->next_sched;
	raise_softirq_irqoff(NET_TX_SOFTIRQ);
	local_irq_restore(flags);
}
	\end{lstlisting}
\end{center}
\FloatBarrier

В этой функции выполняются 2 основных действия: добавляется Qdisc в очередь output\_queue\_tailp на обработку и вызывается NET\_TX\_SOFTIRQ, обработчиком которой является функция net\_tx\_action, код которой представлен в листинге~\ref{lst:net_tx_action}~\cite{net_tx_action}.
\begin{center}
	\captionsetup{justification=raggedright,singlelinecheck=off}
	\begin{lstlisting}[label=lst:net_tx_action,caption=Функция net\_tx\_action,showstringspaces=false]
static __latent_entropy void net_tx_action(struct softirq_action *h)
{
	struct softnet_data *sd = this_cpu_ptr(&softnet_data);
	
	if (sd->completion_queue) {
		struct sk_buff *clist;
		
		local_irq_disable();
		clist = sd->completion_queue;
		sd->completion_queue = NULL;
		local_irq_enable();
		
		while (clist) {
			struct sk_buff *skb = clist;
			
			clist = clist->next;
			
			WARN_ON(refcount_read(&skb->users));
			if (likely(get_kfree_skb_cb(skb)->reason == SKB_CONSUMED))
			trace_consume_skb(skb, net_tx_action);
			else
			trace_kfree_skb(skb, net_tx_action,
			get_kfree_skb_cb(skb)->reason);
			
			if (skb->fclone != SKB_FCLONE_UNAVAILABLE)
			__kfree_skb(skb);
			else
			__napi_kfree_skb(skb,
			get_kfree_skb_cb(skb)->reason);
		}
	}
	
	if (sd->output_queue) {
		struct Qdisc *head;
		
		local_irq_disable();
		head = sd->output_queue;
		sd->output_queue = NULL;
		sd->output_queue_tailp = &sd->output_queue;
		local_irq_enable();
		
		rcu_read_lock();
		
		while (head) {
			struct Qdisc *q = head;
			spinlock_t *root_lock = NULL;
			
			head = head->next_sched;
			
			smp_mb__before_atomic();
			
			if (!(q->flags & TCQ_F_NOLOCK)) {
				root_lock = qdisc_lock(q);
				spin_lock(root_lock);
			} else if (unlikely(test_bit(__QDISC_STATE_DEACTIVATED,
			&q->state))) {
				clear_bit(__QDISC_STATE_SCHED, &q->state);
				continue;
			}
			
			clear_bit(__QDISC_STATE_SCHED, &q->state);
			qdisc_run(q);
			if (root_lock)
			spin_unlock(root_lock);
		}
		
		rcu_read_unlock();
	}
	
	xfrm_dev_backlog(sd);
}
	\end{lstlisting}
\end{center}
\FloatBarrier

Данная функция освобождает completion\_queue структуры softnet\_data и отправляет данные находящиеся в очередях этой структуры. 

completion\_queue это просто список sk\_buff, ожидающих освобождения. Функция dev\_kfree\_skb\_irq может использоваться для добавления skb в эту очередь. Данные буферы не освобождаются драйвером сразу, так как освобождение памяти может занять время, и есть случаи (например, обработчики hardirq), когда код должен выполняться как можно быстрее и возвращаться. Для освобождения по списку проходят циклом и для каждого элемента вызывается \_\_kfree\_skb.

Функция net\_tx\_action планируется на выполнение в двух случаях: в dev\_requeue\_skb, когда возникает коллизия или сетевое устройство занято, или в \_\_qdisc\_run, когда заканчивается квота. Если что--то есть в output\_queue, то начинается обработка цикла. Если очередь требует блокировки то она захватывается, иначе если очередь деактивирован, то снимется бит состояния, запланирован на обработку, и переход к следующему элементу списка. Далее вызывается  qdisc\_run и снимается блокировка, если она использовалась.